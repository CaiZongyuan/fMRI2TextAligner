{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import concurrent.futures\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/nsd'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "nsddata_folder = op.join(data_dir, 'nsd')\n",
    "nsddata_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set subj\n",
    "subj = 'subj01'\n",
    "\n",
    "# if you download the data from NSD webset directly\n",
    "# responses_file = f'./nsd/nsddata/ppdata/{subj}/behav/responses.tsv'\n",
    "# stim_info_file = './nsd/nsddata/experiments/nsd/nsd_stim_info_merged.csv'\n",
    "\n",
    "data_dir = '/root/autodl-tmp/.autodl/Projects/fMRI2TextAligner/data'\n",
    "\n",
    "nsddata_folder = op.join(data_dir, 'nsd')\n",
    "\n",
    "responses_file = op.join(nsddata_folder, 'responses.tsv')\n",
    "\n",
    "# Read the nsd_stim_info_merged.csv file\n",
    "stim_info_file = op.join(nsddata_folder, 'nsd_stim_info_merged.csv')\n",
    "\n",
    "caption_files = {\n",
    "    'val2017': op.join(data_dir, 'coco/annotations/captions_val2017.json'),\n",
    "    'train2017': op.join(data_dir, 'coco/annotations/captions_train2017.json')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get captions: 100%|██████████| 30000/30000 [00:00<00:00, 396167.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# read responses.tsv file\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "responses_df = pd.read_csv(responses_file, sep='\\t', usecols=['SUBJECT', 'SESSION', '73KID'])\n",
    "\n",
    "responses_df['73KID'] = responses_df['73KID'] - 1\n",
    "\n",
    "# Add trial column\n",
    "responses_df['trial'] = (responses_df.groupby('SESSION').cumcount() + 1) % 750\n",
    "responses_df['trial'] = responses_df['trial'].apply(lambda x: x if x != 0 else 750)\n",
    "\n",
    "# Save to nsd_dataset.csv\n",
    "nsd_dataset_file = 'nsd_dataset.csv'\n",
    "responses_df.to_csv(nsd_dataset_file, index=False)\n",
    "\n",
    "stim_info_df = pd.read_csv(stim_info_file, usecols=['cocoId', 'cocoSplit', 'nsdId', 'shared1000'])\n",
    "\n",
    "# Merge data\n",
    "nsd_dataset_df = pd.read_csv(nsd_dataset_file)\n",
    "merged_df = pd.merge(nsd_dataset_df, stim_info_df, left_on='73KID', right_on='nsdId')\n",
    "\n",
    "# Check if there is a missing 73KID\n",
    "missing_73KIDs = set(nsd_dataset_df['73KID']) - set(stim_info_df['nsdId'])\n",
    "if missing_73KIDs:\n",
    "    print(f\"Missing 73KIDs: {missing_73KIDs}\")\n",
    "\n",
    "# Preload captions data\n",
    "def load_captions(caption_file):\n",
    "    with open(caption_file, 'r') as f:\n",
    "        captions_data = json.load(f)\n",
    "    captions_dict = {ann['image_id']: ann['caption'] for ann in captions_data['annotations']}\n",
    "    return captions_dict\n",
    "\n",
    "captions_dict = {split: load_captions(file) for split, file in caption_files.items()}\n",
    "\n",
    "# Get captions using the caption json file of the coco dataset\n",
    "def get_caption(row):\n",
    "    coco_split = row['cocoSplit']\n",
    "    coco_id = row['cocoId']\n",
    "    if coco_split in captions_dict:\n",
    "        return captions_dict[coco_split].get(coco_id, '')\n",
    "    return ''\n",
    "\n",
    "captions = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    captions = list(tqdm(executor.map(get_caption, merged_df.to_dict('records')), total=merged_df.shape[0], desc=\"get captions\"))\n",
    "\n",
    "# Add captions column\n",
    "merged_df['caption'] = captions\n",
    "\n",
    "# Save the final nsd_dataset.csv\n",
    "merged_df.to_csv(nsd_dataset_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need coco image datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image in the nsd_stimuli.hdf5 file\n",
    "nsd_stimuli_file = './nsd/nsddata_stimuli/stimuli/nsd/nsd_stimuli.hdf5'\n",
    "with h5py.File(nsd_stimuli_file, 'r') as f:\n",
    "    imgBrick = f['imgBrick']\n",
    "    \n",
    "    # Randomly select 4 trials\n",
    "    selected_trials = random.sample(range(merged_df.shape[0]), 4)\n",
    "    \n",
    "    for trial in selected_trials:\n",
    "        row = merged_df.iloc[trial]\n",
    "        img_index = row['73KID']  \n",
    "        img_data = imgBrick[img_index]\n",
    "        \n",
    "        # Convert to PIL image\n",
    "        img = Image.fromarray(img_data)\n",
    "        \n",
    "        # Add caption information\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Subject: {row['SUBJECT']}, Session: {row['SESSION']}, Trial: {row['trial']}\\nCaption: {row['caption']}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print(f\"Final dataset saved to {nsd_dataset_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data segmentation and saving completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = 'nsd_dataset.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Extract the caption column and generate the id column\n",
    "captions = df['caption'] \n",
    "ids = range(len(captions))\n",
    "\n",
    "\n",
    "# create new DataFrame\n",
    "new_df = pd.DataFrame({'filepath': ids, 'title': captions})\n",
    "\n",
    "# Split data into train and val\n",
    "train_df = new_df.iloc[:27000]\n",
    "val_df = new_df.iloc[27000:]\n",
    "\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "val_df.to_csv('val.csv', index=False)\n",
    "\n",
    "print(\"Data segmentation and saving completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
